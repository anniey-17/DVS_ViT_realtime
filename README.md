**Project Overview: **
//
Optimizing and understanding vision systems' computational accuracy is crucial as they become increasingly integrated into daily life. Dynamic Vision Sensors (DVS) and Vision Transformers (ViTs) lead computer vision technology with efficient object recognition and image processing. However, DVS data's high computational complexity poses a problem for its real-time implementations. Merging these technologies can enhance vision system performance in dynamic environments and optimize real-time DVS processing.

**Project Description: **
//
In our work, we use a ViT architecture to classify the DVS 128 dataset and compare our results with existing works using Spiking Neural Networks (SNNs). We analyze how our method affects accuracy and loss, experimenting with different DVS-to-ViT input patch sizes.

**Key Results: **
//
Patch Size Comparison: Our results show that a large patch size of 32x32 pixels achieves better accuracy and smaller loss than smaller patches of 4x4 pixels as epochs increase.
Performance: Our method achieved a high 98.4% accuracy and a low 0.22 loss within five epochs, significantly outperforming previous works that average 93.13% accuracy over more epochs.

**Significance: **
//
These results highlight the large potential of Vision Transformers (ViTs) for real-time DVS data classification in applications that require high accuracy, such as autonomous vehicles and surveillance systems.
